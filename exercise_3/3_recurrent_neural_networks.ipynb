{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN)\n",
    "====================\n",
    "\n",
    "In this exercise we will work with Recurrent Neural Networks (RNN). A RNN is class of neural networks where the output not only depends on the current input but also on previous inputs along a given input sequence. This allows to exhibit temporal dynamic behaviour and contextual information in a sequence. Common applications for RNN are:\n",
    "\n",
    "- time series analysis\n",
    "- speech recognition\n",
    "- machine translation\n",
    "- image captioning\n",
    "\n",
    "\n",
    "Goal of this exercise\n",
    "========\n",
    "\n",
    "This exercise notebook should help you to experiment how recurrent neural networks are implemented, trained, and used for computer vision problems. Therefore, this notebook is structured as follows:\n",
    "1. Implement your own simple RNN class in Pytorch.\n",
    "2. Explore the backpropagation of the gradients in the RNN and discuss the vanishing gradient problem.\n",
    "3. Implement your own LSTM (Long-Short Term Memory) Network and show that this architecture improves the vanishing gradient problem.\n",
    "4. Build a RNN classifier for the MNIST dataset and train your model.\n",
    "5. Tune the hyperparameters of your model and submit your best model to the server to get bonus points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python:  3.7.5\n",
      "Using torch version:  1.0.0\n",
      "Using device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print('Using python: ', platform.python_version())\n",
    "print('Using torch version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "# Machine: 2015 13\" Macbook Pro, i5 dual core"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Recurrent Neural Network\n",
    "\n",
    "The recurrent loops in a RNN allow relevant information to persist over time. A simple RNN architecture is shown here:\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png width=\"150\">\n",
    "\n",
    "A simple RNN takes not only an input X at time step t but also passes a hidden state that is the output of the previous time step into the network. The output of a RNN cell at time step t reads in Eq. 1:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "In this task you have to implement a simple one-layer RNN as a class in Pytorch, where you can choose a relu or tanh activation in the cell.You can see the architecture of a simple RNN in the figure below.\n",
    "\n",
    "\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Implement the RNN class\n",
    "from exercise_code.rnn.rnn_nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, Pytorch already has implemented a simple RNN in their library and you can call the RNN with <code>nn.RNN</code>. We will use the Pytorch RNN function to check if we have built the correct cell and compare the output of both functions. We also compare the running time of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differnce between pytorch and your RNN implementation: 0.0\n",
      "Cool, you implemented a correct model.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import timeit\n",
    "\n",
    "# choose your network parameters\n",
    "input_size = 3\n",
    "hidden_dim = 3\n",
    "seq_len = 10 \n",
    "\n",
    "# define the two models\n",
    "pytorch_rnn=nn.RNN(input_size, hidden_dim)\n",
    "i2dl_rnn=RNN(input_size, hidden_dim)\n",
    "\n",
    "# initialise both rnn with same values\n",
    "for p in pytorch_rnn.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "for p in i2dl_rnn.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "    \n",
    "X=torch.randn(seq_len, 1, input_size)\n",
    "\n",
    "output_pytorch, h_pytorch = pytorch_rnn(X)\n",
    "output_i2dl, h_i2dl = i2dl_rnn(X)\n",
    "\n",
    "\n",
    "# The difference of outputs should be 0!!\n",
    "\n",
    "diff = torch.sum((output_pytorch-output_i2dl) ** 2)\n",
    "print(\"Differnce between pytorch and your RNN implementation: %s\" % diff.item())\n",
    "if diff.item() < 10 ** -10:\n",
    "    print(\"Cool, you implemented a correct model.\")\n",
    "else:\n",
    "    print(\"Upps! There is something wrong in your model. Try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Pytorch RNN 10000 runs: 3.039s\n",
      "Time I2DL RNN 10000 run: 11.112s\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "runs=10 ** 4\n",
    "\n",
    "print(\"Time Pytorch RNN {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_rnn(X)\", \n",
    "                                       setup=\"from __main__ import pytorch_rnn, X\", \n",
    "                                       number=runs))\n",
    "     )\n",
    "\n",
    "print(\"Time I2DL RNN {} run: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_rnn(X)\", \n",
    "                                       setup=\"from __main__ import i2dl_rnn, X\", \n",
    "                                       number=runs))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will use the Pytorch module that is faster and optimised in performance. However, it is always a good exercise to build the functions by yourself and we really advice you to do the exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient\n",
    "\n",
    "As discussed in the lecture, the simple RNN suffers from vanishing gradients in the backpropagation. The hidden state is manipulated in every time step along the sequence and the effect of the past inputs to the final output vanishes with the distance in time. In the next cell we will explore the vanishing effect of previous inputs in the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# TODO: Define a RNN and explore the gradients on the output h_T wrt. the  #\n",
    "# input at time t and plot your result. What behaviour do you observe?     #\n",
    "# Hints:                                                                   #\n",
    "#   - use one input feature                                                #\n",
    "#   - pytorch allows backward() pass wrt. to any vector                    #\n",
    "#   - backward() can only be applied to scalars and not to output tensors  #\n",
    "#   - choose a good representation of the gradient plot                    #\n",
    "############################################################################\n",
    " \n",
    "    \n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>It can be seen that the gradient of the output at time t wrt. to a previous input decreases exponentially. Hence, the final output does not change significantly for changes in the previous input and hence the RNN does not have memory.</p> \n",
    "<h3>Question</h3> \n",
    "<p>In order to better understand the vanishing gradient problem, calculate the gradients \n",
    "dh_t/dV, dh_t /dW, and dh_t/dX_0 analytically for t=3 and h_0=0 using Eq. 1. This exercise might seem a little bit tedious but it is really useful. Can you explain the vanishing gradient mathematically based on your findings?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Short Term Memory Network (LSTM)\n",
    "The vanishing gradient problem had been known for some time until Schmidhuber (1997) developed the Long-Short Term Memory Network and showed that this architecture can overcome the problem. <br> \n",
    "A LSTM is a more advanced recurrent network architecture that is able to learn long time dependencies. The architecture of a LSTM is composed of a forget, input, and output gate and the cell can remember values over arbitrary time intervals. Despite various different and exotic LSTM architectures, the standard LSTM cell is shwon in the figure below:\n",
    "\n",
    "\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to a simple RNN the LSTM cell has a hidden vector and an additional cell state vector. __What size does the cell state have?__ <br>\n",
    "The operations inside the LSTM are given as \n",
    "\n",
    "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1  width=\"600\">\n",
    "where \n",
    "f_t: forget gate,  <br>\n",
    "i_t: input gate, <br>\n",
    "o_t: output gate, <br>\n",
    "h_t: hidden state vector, <br>\n",
    "c_t: cell state vector, <br>\n",
    "x_t: input vector, <br>\n",
    "t is time step, \n",
    "<br> \n",
    "<br> \n",
    "and<br> \n",
    "sigma_g: sigmoid activation <br> \n",
    "sigma_c and sigma_h: hyperbolic tangent function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step you should implement your own LSTM with the operations stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Implement the RNN class\n",
    "from exercise_code.rnn.rnn_nn import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differnce between pytorch and your RNN implementation: 0.0\n",
      "Cool, you implemented a correct model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# choose your input parameters\n",
    "input_size = 3\n",
    "hidden_dim = 3\n",
    "seq_len = 10 \n",
    "\n",
    "# define the two models\n",
    "pytorch_lstm=nn.LSTM(input_size, hidden_dim)\n",
    "i2dl_lstm=LSTM(input_size, hidden_dim)\n",
    "\n",
    "# initialise both lstms with same values\n",
    "for p in pytorch_lstm.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "for p in i2dl_lstm.parameters():\n",
    "    nn.init.constant_(p, val=0.3)\n",
    "    \n",
    "X=torch.randn(seq_len, 1, input_size)\n",
    "\n",
    "output_pytorch, (h_pytorch, _) = pytorch_lstm(X)\n",
    "output_i2dl , (h_i2dl, _ )= i2dl_lstm(X)\n",
    "\n",
    "# The difference of outputs should be 0!!\n",
    "diff = torch.sum((output_pytorch-output_i2dl) ** 2)\n",
    "print(\"Differnce between pytorch and your RNN implementation: %s\" % diff.item())\n",
    "if diff.item() < 10 ** -10:\n",
    "    print(\"Cool, you implemented a correct model.\")\n",
    "else:\n",
    "    print(\"Upps! There is something wrong in your model. Try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Pytorch LSTM 10000 runs: 6.836s\n",
      "Time I2DL LSTM 10000 runs: 44.756s\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "runs=10 ** 4\n",
    "\n",
    "print(\"Time Pytorch LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_lstm(X)\", \n",
    "                                       setup=\"from __main__ import pytorch_lstm, X\", \n",
    "                                       number=runs))\n",
    "     )\n",
    "\n",
    "print(\"Time I2DL LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_lstm(X)\", \n",
    "                                       setup=\"from __main__ import i2dl_lstm, X\", \n",
    "                                       number=runs))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Gradients \n",
    "Analogously to the RNN, calculate the gradients of the input wrt. to the output of the LSTM and compare it against the RNN gradients. __What do you see?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# TODO: Define a RNN and LSTM and explore the gradients on the output h_T   #\n",
    "# wrt. the input at time t and plot your result.                           #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST image classification with RNNs\n",
    "\n",
    "In the previous exercises we already have classified images with a Fully Connected and Convolutional Network. In this exercise, we will solve the problem of image classification with a recurrent neural network.  \n",
    "\n",
    "For the experiment we use the MNIST handwritten digits dataset which we already know from the autoencoder exercise. This dataset consists of images of the 10 different digits (10 classes). The images have the resolution 28 x 28. The idea for the RNN classifier is to interpret the image as a sequence of rows. This means that we pass the rows through the RNN and use the final hidden state for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>\n",
    "    In this semester you have seen three different types of neural networks, namely Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and now Recurrent Neural Networks (RNNs). We have seen that we can use all three architectures for image classification. However, it turned out that some models are better than others for image classification. Try to think about advantages and disadvantages of the models, regarding # of parameters, transformations of the object in the image (scaling, rotation, translation,...), training time, testing time, over-fitting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "\n",
    "class Unsqueeze(object):\n",
    "    \"\"\"Adds a channel dimension that that our 2 dimensional input (H, W), \n",
    "    fits the 3 dimensional (H, W, C) expectations of pytorch's ToTensor function which\n",
    "    expects a PIL image. This is very inefficient but you most probably will use pytorch's\n",
    "    PIL image loader. Check out the documentation and make it more efficient :)\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension=0):\n",
    "        self.dimension = dimension\n",
    "    def __call__(self, numpy_array):\n",
    "        extended_array = np.expand_dims(numpy_array, self.dimension)\n",
    "        return extended_array\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + 'dimension={}'.format(dimension)\n",
    "\n",
    "    \n",
    "# transformation of data\n",
    "transform = transforms.Compose([\n",
    "    Unsqueeze(dimension=3),     \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "class MnistDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, \n",
    "                 transform=None):\n",
    "        super(MnistDataset, self).__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "# loading the train data\n",
    "with open(\"../datasets/mnist/mnist_train.p\", \"rb\") as f:\n",
    "    mnist_raw = pickle.load(f)\n",
    "\n",
    "X, y = mnist_raw\n",
    "############################################################################\n",
    "# TODO: Set a useful training/ validation split                            #\n",
    "############################################################################    \n",
    "\n",
    "train_split = 0.85\n",
    "\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "train_dset = MnistDataset(X[:int(len(X) * train_split)], y[:int(len(X) * train_split)], transform=transform)\n",
    "val_dset = MnistDataset(X[int(len(X) * train_split):], y[int(len(X) * train_split):], transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 50 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some examples from the dataset. Stolen from other notebooks\n",
    "# We show a few examples of training images from each class.\n",
    "X=train_dset.images\n",
    "y=train_dset.labels\n",
    "\n",
    "\n",
    "classes = list(range(10))\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 5\n",
    "for y_hat, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(train_dset.labels == y_hat)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y_hat + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier based on a RNN where you sequentially feed the rows in the network and use the final hidden state for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://cdn-images-1.medium.com/max/800/1*Cm_c-I02rBa1rtLZXBhNUw.png width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.rnn.rnn_nn import LSTM_Classifier, RNN_Classifier\n",
    "model_rnn = LSTM_Classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.rnn.solver import Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/miniconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 50/15940] TRAIN loss: 2.262\n",
      "[Iteration 100/15940] TRAIN loss: 1.493\n",
      "[Iteration 150/15940] TRAIN loss: 0.842\n",
      "[Iteration 200/15940] TRAIN loss: 0.596\n",
      "[Iteration 250/15940] TRAIN loss: 0.515\n",
      "[Iteration 300/15940] TRAIN loss: 0.387\n",
      "[Iteration 350/15940] TRAIN loss: 0.369\n",
      "[Iteration 400/15940] TRAIN loss: 0.347\n",
      "[Iteration 450/15940] TRAIN loss: 0.312\n",
      "[Iteration 500/15940] TRAIN loss: 0.280\n",
      "[Iteration 550/15940] TRAIN loss: 0.226\n",
      "[Iteration 600/15940] TRAIN loss: 0.242\n",
      "[Iteration 650/15940] TRAIN loss: 0.303\n",
      "[Iteration 700/15940] TRAIN loss: 0.236\n",
      "[Iteration 750/15940] TRAIN loss: 0.233\n",
      "[Iteration 800/15940] TRAIN loss: 0.219\n",
      "[Iteration 850/15940] TRAIN loss: 0.175\n",
      "[Iteration 900/15940] TRAIN loss: 0.182\n",
      "[Iteration 950/15940] TRAIN loss: 0.207\n",
      "[Iteration 1000/15940] TRAIN loss: 0.178\n",
      "[Iteration 1050/15940] TRAIN loss: 0.170\n",
      "[Iteration 1100/15940] TRAIN loss: 0.198\n",
      "[Iteration 1150/15940] TRAIN loss: 0.150\n",
      "[Iteration 1200/15940] TRAIN loss: 0.177\n",
      "[Iteration 1250/15940] TRAIN loss: 0.165\n",
      "[Iteration 1300/15940] TRAIN loss: 0.158\n",
      "[Iteration 1350/15940] TRAIN loss: 0.166\n",
      "[Iteration 1400/15940] TRAIN loss: 0.166\n",
      "[Iteration 1450/15940] TRAIN loss: 0.138\n",
      "[Iteration 1500/15940] TRAIN loss: 0.143\n",
      "[Iteration 1550/15940] TRAIN loss: 0.131\n",
      "[Epoch 1/10] TRAIN acc/loss: 1.000/0.131\n",
      "[Epoch 1/10] VAL   acc/loss: 0.962/0.123\n",
      "[Iteration 1644/15940] TRAIN loss: 0.133\n",
      "[Iteration 1694/15940] TRAIN loss: 0.112\n",
      "[Iteration 1744/15940] TRAIN loss: 0.124\n",
      "[Iteration 1794/15940] TRAIN loss: 0.104\n",
      "[Iteration 1844/15940] TRAIN loss: 0.135\n",
      "[Iteration 1894/15940] TRAIN loss: 0.111\n",
      "[Iteration 1944/15940] TRAIN loss: 0.106\n",
      "[Iteration 1994/15940] TRAIN loss: 0.110\n",
      "[Iteration 2044/15940] TRAIN loss: 0.101\n",
      "[Iteration 2094/15940] TRAIN loss: 0.132\n",
      "[Iteration 2144/15940] TRAIN loss: 0.118\n",
      "[Iteration 2194/15940] TRAIN loss: 0.109\n",
      "[Iteration 2244/15940] TRAIN loss: 0.111\n",
      "[Iteration 2294/15940] TRAIN loss: 0.124\n",
      "[Iteration 2344/15940] TRAIN loss: 0.105\n",
      "[Iteration 2394/15940] TRAIN loss: 0.094\n",
      "[Iteration 2444/15940] TRAIN loss: 0.107\n",
      "[Iteration 2494/15940] TRAIN loss: 0.112\n",
      "[Iteration 2544/15940] TRAIN loss: 0.109\n",
      "[Iteration 2594/15940] TRAIN loss: 0.101\n",
      "[Iteration 2644/15940] TRAIN loss: 0.104\n",
      "[Iteration 2694/15940] TRAIN loss: 0.080\n",
      "[Iteration 2744/15940] TRAIN loss: 0.087\n",
      "[Iteration 2794/15940] TRAIN loss: 0.113\n",
      "[Iteration 2844/15940] TRAIN loss: 0.086\n",
      "[Iteration 2894/15940] TRAIN loss: 0.078\n",
      "[Iteration 2944/15940] TRAIN loss: 0.116\n",
      "[Iteration 2994/15940] TRAIN loss: 0.113\n",
      "[Iteration 3044/15940] TRAIN loss: 0.095\n",
      "[Iteration 3094/15940] TRAIN loss: 0.122\n",
      "[Iteration 3144/15940] TRAIN loss: 0.096\n",
      "[Epoch 2/10] TRAIN acc/loss: 1.000/0.096\n",
      "[Epoch 2/10] VAL   acc/loss: 0.974/0.088\n",
      "[Iteration 3238/15940] TRAIN loss: 0.089\n",
      "[Iteration 3288/15940] TRAIN loss: 0.095\n",
      "[Iteration 3338/15940] TRAIN loss: 0.086\n",
      "[Iteration 3388/15940] TRAIN loss: 0.083\n",
      "[Iteration 3438/15940] TRAIN loss: 0.091\n",
      "[Iteration 3488/15940] TRAIN loss: 0.052\n",
      "[Iteration 3538/15940] TRAIN loss: 0.077\n",
      "[Iteration 3588/15940] TRAIN loss: 0.094\n",
      "[Iteration 3638/15940] TRAIN loss: 0.095\n",
      "[Iteration 3688/15940] TRAIN loss: 0.084\n",
      "[Iteration 3738/15940] TRAIN loss: 0.072\n",
      "[Iteration 3788/15940] TRAIN loss: 0.080\n",
      "[Iteration 3838/15940] TRAIN loss: 0.053\n",
      "[Iteration 3888/15940] TRAIN loss: 0.064\n",
      "[Iteration 3938/15940] TRAIN loss: 0.087\n",
      "[Iteration 3988/15940] TRAIN loss: 0.073\n",
      "[Iteration 4038/15940] TRAIN loss: 0.061\n",
      "[Iteration 4088/15940] TRAIN loss: 0.086\n",
      "[Iteration 4138/15940] TRAIN loss: 0.093\n",
      "[Iteration 4188/15940] TRAIN loss: 0.084\n",
      "[Iteration 4238/15940] TRAIN loss: 0.058\n",
      "[Iteration 4288/15940] TRAIN loss: 0.048\n",
      "[Iteration 4338/15940] TRAIN loss: 0.086\n",
      "[Iteration 4388/15940] TRAIN loss: 0.073\n",
      "[Iteration 4438/15940] TRAIN loss: 0.073\n",
      "[Iteration 4488/15940] TRAIN loss: 0.063\n",
      "[Iteration 4538/15940] TRAIN loss: 0.050\n",
      "[Iteration 4588/15940] TRAIN loss: 0.080\n",
      "[Iteration 4638/15940] TRAIN loss: 0.084\n",
      "[Iteration 4688/15940] TRAIN loss: 0.075\n",
      "[Iteration 4738/15940] TRAIN loss: 0.060\n",
      "[Epoch 3/10] TRAIN acc/loss: 1.000/0.060\n",
      "[Epoch 3/10] VAL   acc/loss: 0.979/0.069\n",
      "[Iteration 4832/15940] TRAIN loss: 0.063\n",
      "[Iteration 4882/15940] TRAIN loss: 0.049\n",
      "[Iteration 4932/15940] TRAIN loss: 0.074\n",
      "[Iteration 4982/15940] TRAIN loss: 0.062\n",
      "[Iteration 5032/15940] TRAIN loss: 0.048\n",
      "[Iteration 5082/15940] TRAIN loss: 0.085\n",
      "[Iteration 5132/15940] TRAIN loss: 0.057\n",
      "[Iteration 5182/15940] TRAIN loss: 0.061\n",
      "[Iteration 5232/15940] TRAIN loss: 0.051\n",
      "[Iteration 5282/15940] TRAIN loss: 0.073\n",
      "[Iteration 5332/15940] TRAIN loss: 0.049\n",
      "[Iteration 5382/15940] TRAIN loss: 0.067\n",
      "[Iteration 5432/15940] TRAIN loss: 0.047\n",
      "[Iteration 5482/15940] TRAIN loss: 0.041\n",
      "[Iteration 5532/15940] TRAIN loss: 0.067\n",
      "[Iteration 5582/15940] TRAIN loss: 0.058\n",
      "[Iteration 5632/15940] TRAIN loss: 0.058\n",
      "[Iteration 5682/15940] TRAIN loss: 0.053\n",
      "[Iteration 5732/15940] TRAIN loss: 0.055\n",
      "[Iteration 5782/15940] TRAIN loss: 0.066\n",
      "[Iteration 5832/15940] TRAIN loss: 0.063\n",
      "[Iteration 5882/15940] TRAIN loss: 0.069\n",
      "[Iteration 5932/15940] TRAIN loss: 0.060\n",
      "[Iteration 5982/15940] TRAIN loss: 0.077\n",
      "[Iteration 6032/15940] TRAIN loss: 0.051\n",
      "[Iteration 6082/15940] TRAIN loss: 0.059\n",
      "[Iteration 6132/15940] TRAIN loss: 0.082\n",
      "[Iteration 6182/15940] TRAIN loss: 0.074\n",
      "[Iteration 6232/15940] TRAIN loss: 0.063\n",
      "[Iteration 6282/15940] TRAIN loss: 0.076\n",
      "[Iteration 6332/15940] TRAIN loss: 0.053\n",
      "[Epoch 4/10] TRAIN acc/loss: 0.958/0.053\n",
      "[Epoch 4/10] VAL   acc/loss: 0.979/0.076\n",
      "[Iteration 6426/15940] TRAIN loss: 0.047\n",
      "[Iteration 6476/15940] TRAIN loss: 0.051\n",
      "[Iteration 6526/15940] TRAIN loss: 0.055\n",
      "[Iteration 6576/15940] TRAIN loss: 0.034\n",
      "[Iteration 6626/15940] TRAIN loss: 0.047\n",
      "[Iteration 6676/15940] TRAIN loss: 0.057\n",
      "[Iteration 6726/15940] TRAIN loss: 0.060\n",
      "[Iteration 6776/15940] TRAIN loss: 0.056\n",
      "[Iteration 6826/15940] TRAIN loss: 0.049\n",
      "[Iteration 6876/15940] TRAIN loss: 0.059\n",
      "[Iteration 6926/15940] TRAIN loss: 0.055\n",
      "[Iteration 6976/15940] TRAIN loss: 0.057\n",
      "[Iteration 7026/15940] TRAIN loss: 0.075\n",
      "[Iteration 7076/15940] TRAIN loss: 0.063\n",
      "[Iteration 7126/15940] TRAIN loss: 0.076\n",
      "[Iteration 7176/15940] TRAIN loss: 0.051\n",
      "[Iteration 7226/15940] TRAIN loss: 0.046\n",
      "[Iteration 7276/15940] TRAIN loss: 0.044\n",
      "[Iteration 7326/15940] TRAIN loss: 0.042\n",
      "[Iteration 7376/15940] TRAIN loss: 0.045\n",
      "[Iteration 7426/15940] TRAIN loss: 0.054\n",
      "[Iteration 7476/15940] TRAIN loss: 0.064\n",
      "[Iteration 7526/15940] TRAIN loss: 0.037\n",
      "[Iteration 7576/15940] TRAIN loss: 0.045\n",
      "[Iteration 7626/15940] TRAIN loss: 0.059\n",
      "[Iteration 7676/15940] TRAIN loss: 0.039\n",
      "[Iteration 7726/15940] TRAIN loss: 0.038\n",
      "[Iteration 7776/15940] TRAIN loss: 0.048\n",
      "[Iteration 7826/15940] TRAIN loss: 0.035\n",
      "[Iteration 7876/15940] TRAIN loss: 0.062\n",
      "[Iteration 7926/15940] TRAIN loss: 0.063\n",
      "[Epoch 5/10] TRAIN acc/loss: 1.000/0.063\n",
      "[Epoch 5/10] VAL   acc/loss: 0.979/0.065\n",
      "[Iteration 8020/15940] TRAIN loss: 0.036\n",
      "[Iteration 8070/15940] TRAIN loss: 0.034\n",
      "[Iteration 8120/15940] TRAIN loss: 0.049\n",
      "[Iteration 8170/15940] TRAIN loss: 0.030\n",
      "[Iteration 8220/15940] TRAIN loss: 0.028\n",
      "[Iteration 8270/15940] TRAIN loss: 0.035\n",
      "[Iteration 8320/15940] TRAIN loss: 0.051\n",
      "[Iteration 8370/15940] TRAIN loss: 0.057\n",
      "[Iteration 8420/15940] TRAIN loss: 0.047\n",
      "[Iteration 8470/15940] TRAIN loss: 0.044\n",
      "[Iteration 8520/15940] TRAIN loss: 0.030\n",
      "[Iteration 8570/15940] TRAIN loss: 0.045\n",
      "[Iteration 8620/15940] TRAIN loss: 0.044\n",
      "[Iteration 8670/15940] TRAIN loss: 0.067\n",
      "[Iteration 8720/15940] TRAIN loss: 0.039\n",
      "[Iteration 8770/15940] TRAIN loss: 0.049\n",
      "[Iteration 8820/15940] TRAIN loss: 0.046\n",
      "[Iteration 8870/15940] TRAIN loss: 0.048\n",
      "[Iteration 8920/15940] TRAIN loss: 0.053\n",
      "[Iteration 8970/15940] TRAIN loss: 0.029\n",
      "[Iteration 9020/15940] TRAIN loss: 0.046\n",
      "[Iteration 9070/15940] TRAIN loss: 0.046\n",
      "[Iteration 9120/15940] TRAIN loss: 0.033\n",
      "[Iteration 9170/15940] TRAIN loss: 0.061\n",
      "[Iteration 9220/15940] TRAIN loss: 0.038\n",
      "[Iteration 9270/15940] TRAIN loss: 0.046\n",
      "[Iteration 9320/15940] TRAIN loss: 0.053\n",
      "[Iteration 9370/15940] TRAIN loss: 0.046\n",
      "[Iteration 9420/15940] TRAIN loss: 0.058\n",
      "[Iteration 9470/15940] TRAIN loss: 0.056\n",
      "[Iteration 9520/15940] TRAIN loss: 0.056\n",
      "[Epoch 6/10] TRAIN acc/loss: 1.000/0.056\n",
      "[Epoch 6/10] VAL   acc/loss: 0.982/0.057\n",
      "[Iteration 9614/15940] TRAIN loss: 0.031\n",
      "[Iteration 9664/15940] TRAIN loss: 0.030\n",
      "[Iteration 9714/15940] TRAIN loss: 0.032\n",
      "[Iteration 9764/15940] TRAIN loss: 0.041\n",
      "[Iteration 9814/15940] TRAIN loss: 0.058\n",
      "[Iteration 9864/15940] TRAIN loss: 0.031\n",
      "[Iteration 9914/15940] TRAIN loss: 0.032\n",
      "[Iteration 9964/15940] TRAIN loss: 0.034\n",
      "[Iteration 10014/15940] TRAIN loss: 0.021\n",
      "[Iteration 10064/15940] TRAIN loss: 0.039\n",
      "[Iteration 10114/15940] TRAIN loss: 0.033\n",
      "[Iteration 10164/15940] TRAIN loss: 0.036\n",
      "[Iteration 10214/15940] TRAIN loss: 0.036\n",
      "[Iteration 10264/15940] TRAIN loss: 0.043\n",
      "[Iteration 10314/15940] TRAIN loss: 0.044\n",
      "[Iteration 10364/15940] TRAIN loss: 0.047\n",
      "[Iteration 10414/15940] TRAIN loss: 0.036\n",
      "[Iteration 10464/15940] TRAIN loss: 0.033\n",
      "[Iteration 10514/15940] TRAIN loss: 0.041\n",
      "[Iteration 10564/15940] TRAIN loss: 0.037\n",
      "[Iteration 10614/15940] TRAIN loss: 0.031\n",
      "[Iteration 10664/15940] TRAIN loss: 0.033\n",
      "[Iteration 10714/15940] TRAIN loss: 0.037\n",
      "[Iteration 10764/15940] TRAIN loss: 0.031\n",
      "[Iteration 10814/15940] TRAIN loss: 0.043\n",
      "[Iteration 10864/15940] TRAIN loss: 0.047\n",
      "[Iteration 10914/15940] TRAIN loss: 0.034\n",
      "[Iteration 10964/15940] TRAIN loss: 0.052\n",
      "[Iteration 11014/15940] TRAIN loss: 0.054\n",
      "[Iteration 11064/15940] TRAIN loss: 0.054\n",
      "[Iteration 11114/15940] TRAIN loss: 0.048\n",
      "[Epoch 7/10] TRAIN acc/loss: 1.000/0.048\n",
      "[Epoch 7/10] VAL   acc/loss: 0.985/0.051\n",
      "[Iteration 11208/15940] TRAIN loss: 0.023\n",
      "[Iteration 11258/15940] TRAIN loss: 0.032\n",
      "[Iteration 11308/15940] TRAIN loss: 0.024\n",
      "[Iteration 11358/15940] TRAIN loss: 0.032\n",
      "[Iteration 11408/15940] TRAIN loss: 0.034\n",
      "[Iteration 11458/15940] TRAIN loss: 0.024\n",
      "[Iteration 11508/15940] TRAIN loss: 0.034\n",
      "[Iteration 11558/15940] TRAIN loss: 0.017\n",
      "[Iteration 11608/15940] TRAIN loss: 0.035\n",
      "[Iteration 11658/15940] TRAIN loss: 0.049\n",
      "[Iteration 11708/15940] TRAIN loss: 0.019\n",
      "[Iteration 11758/15940] TRAIN loss: 0.035\n",
      "[Iteration 11808/15940] TRAIN loss: 0.032\n",
      "[Iteration 11858/15940] TRAIN loss: 0.020\n",
      "[Iteration 11908/15940] TRAIN loss: 0.044\n",
      "[Iteration 11958/15940] TRAIN loss: 0.035\n",
      "[Iteration 12008/15940] TRAIN loss: 0.032\n",
      "[Iteration 12058/15940] TRAIN loss: 0.037\n",
      "[Iteration 12108/15940] TRAIN loss: 0.027\n",
      "[Iteration 12158/15940] TRAIN loss: 0.037\n",
      "[Iteration 12208/15940] TRAIN loss: 0.034\n",
      "[Iteration 12258/15940] TRAIN loss: 0.039\n",
      "[Iteration 12308/15940] TRAIN loss: 0.050\n",
      "[Iteration 12358/15940] TRAIN loss: 0.039\n",
      "[Iteration 12408/15940] TRAIN loss: 0.038\n",
      "[Iteration 12458/15940] TRAIN loss: 0.055\n",
      "[Iteration 12508/15940] TRAIN loss: 0.037\n",
      "[Iteration 12558/15940] TRAIN loss: 0.035\n",
      "[Iteration 12608/15940] TRAIN loss: 0.029\n",
      "[Iteration 12658/15940] TRAIN loss: 0.029\n",
      "[Iteration 12708/15940] TRAIN loss: 0.024\n",
      "[Epoch 8/10] TRAIN acc/loss: 1.000/0.024\n",
      "[Epoch 8/10] VAL   acc/loss: 0.985/0.048\n",
      "[Iteration 12802/15940] TRAIN loss: 0.021\n",
      "[Iteration 12852/15940] TRAIN loss: 0.016\n",
      "[Iteration 12902/15940] TRAIN loss: 0.037\n",
      "[Iteration 12952/15940] TRAIN loss: 0.036\n",
      "[Iteration 13002/15940] TRAIN loss: 0.034\n",
      "[Iteration 13052/15940] TRAIN loss: 0.019\n",
      "[Iteration 13102/15940] TRAIN loss: 0.029\n",
      "[Iteration 13152/15940] TRAIN loss: 0.030\n",
      "[Iteration 13202/15940] TRAIN loss: 0.035\n",
      "[Iteration 13252/15940] TRAIN loss: 0.045\n",
      "[Iteration 13302/15940] TRAIN loss: 0.033\n",
      "[Iteration 13352/15940] TRAIN loss: 0.048\n",
      "[Iteration 13402/15940] TRAIN loss: 0.032\n",
      "[Iteration 13452/15940] TRAIN loss: 0.028\n",
      "[Iteration 13502/15940] TRAIN loss: 0.016\n",
      "[Iteration 13552/15940] TRAIN loss: 0.024\n",
      "[Iteration 13602/15940] TRAIN loss: 0.018\n",
      "[Iteration 13652/15940] TRAIN loss: 0.016\n",
      "[Iteration 13702/15940] TRAIN loss: 0.024\n",
      "[Iteration 13752/15940] TRAIN loss: 0.023\n",
      "[Iteration 13802/15940] TRAIN loss: 0.032\n",
      "[Iteration 13852/15940] TRAIN loss: 0.036\n",
      "[Iteration 13902/15940] TRAIN loss: 0.036\n",
      "[Iteration 13952/15940] TRAIN loss: 0.025\n",
      "[Iteration 14002/15940] TRAIN loss: 0.024\n",
      "[Iteration 14052/15940] TRAIN loss: 0.021\n",
      "[Iteration 14102/15940] TRAIN loss: 0.027\n",
      "[Iteration 14152/15940] TRAIN loss: 0.041\n",
      "[Iteration 14202/15940] TRAIN loss: 0.040\n",
      "[Iteration 14252/15940] TRAIN loss: 0.040\n",
      "[Iteration 14302/15940] TRAIN loss: 0.032\n",
      "[Epoch 9/10] TRAIN acc/loss: 1.000/0.032\n",
      "[Epoch 9/10] VAL   acc/loss: 0.985/0.050\n",
      "[Iteration 14396/15940] TRAIN loss: 0.023\n",
      "[Iteration 14446/15940] TRAIN loss: 0.019\n",
      "[Iteration 14496/15940] TRAIN loss: 0.030\n",
      "[Iteration 14546/15940] TRAIN loss: 0.011\n",
      "[Iteration 14596/15940] TRAIN loss: 0.014\n",
      "[Iteration 14646/15940] TRAIN loss: 0.020\n",
      "[Iteration 14696/15940] TRAIN loss: 0.018\n",
      "[Iteration 14746/15940] TRAIN loss: 0.012\n",
      "[Iteration 14796/15940] TRAIN loss: 0.026\n",
      "[Iteration 14846/15940] TRAIN loss: 0.030\n",
      "[Iteration 14896/15940] TRAIN loss: 0.029\n",
      "[Iteration 14946/15940] TRAIN loss: 0.043\n",
      "[Iteration 14996/15940] TRAIN loss: 0.030\n",
      "[Iteration 15046/15940] TRAIN loss: 0.032\n",
      "[Iteration 15096/15940] TRAIN loss: 0.022\n",
      "[Iteration 15146/15940] TRAIN loss: 0.033\n",
      "[Iteration 15196/15940] TRAIN loss: 0.018\n",
      "[Iteration 15246/15940] TRAIN loss: 0.023\n",
      "[Iteration 15296/15940] TRAIN loss: 0.034\n",
      "[Iteration 15346/15940] TRAIN loss: 0.034\n",
      "[Iteration 15396/15940] TRAIN loss: 0.026\n",
      "[Iteration 15446/15940] TRAIN loss: 0.008\n",
      "[Iteration 15496/15940] TRAIN loss: 0.027\n",
      "[Iteration 15546/15940] TRAIN loss: 0.036\n",
      "[Iteration 15596/15940] TRAIN loss: 0.039\n",
      "[Iteration 15646/15940] TRAIN loss: 0.033\n",
      "[Iteration 15696/15940] TRAIN loss: 0.041\n",
      "[Iteration 15746/15940] TRAIN loss: 0.019\n",
      "[Iteration 15796/15940] TRAIN loss: 0.025\n",
      "[Iteration 15846/15940] TRAIN loss: 0.030\n",
      "[Iteration 15896/15940] TRAIN loss: 0.041\n",
      "[Epoch 10/10] TRAIN acc/loss: 1.000/0.041\n",
      "[Epoch 10/10] VAL   acc/loss: 0.987/0.048\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_dset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                dataset=val_dset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "solver = Solver(optim_args={\"lr\": 1e-3})\n",
    "\n",
    "# train rnn model\n",
    "solver.train(model_rnn, train_loader, val_loader, log_nth=50, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your RNN classifier and try to tune the hyperparameters. With your simple RNN classifier you should exceed an accuracy higher than __90%__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve your model by using a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/miniconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 50/15940] TRAIN loss: 2.202\n",
      "[Iteration 100/15940] TRAIN loss: 1.581\n",
      "[Iteration 150/15940] TRAIN loss: 1.094\n",
      "[Iteration 200/15940] TRAIN loss: 0.786\n",
      "[Iteration 250/15940] TRAIN loss: 0.725\n",
      "[Iteration 300/15940] TRAIN loss: 0.534\n",
      "[Iteration 350/15940] TRAIN loss: 0.465\n",
      "[Iteration 400/15940] TRAIN loss: 0.393\n",
      "[Iteration 450/15940] TRAIN loss: 0.416\n",
      "[Iteration 500/15940] TRAIN loss: 0.301\n",
      "[Iteration 550/15940] TRAIN loss: 0.354\n",
      "[Iteration 600/15940] TRAIN loss: 0.294\n",
      "[Iteration 650/15940] TRAIN loss: 0.299\n",
      "[Iteration 700/15940] TRAIN loss: 0.272\n",
      "[Iteration 750/15940] TRAIN loss: 0.242\n",
      "[Iteration 800/15940] TRAIN loss: 0.248\n",
      "[Iteration 850/15940] TRAIN loss: 0.211\n",
      "[Iteration 900/15940] TRAIN loss: 0.227\n",
      "[Iteration 950/15940] TRAIN loss: 0.259\n",
      "[Iteration 1000/15940] TRAIN loss: 0.214\n",
      "[Iteration 1050/15940] TRAIN loss: 0.176\n",
      "[Iteration 1100/15940] TRAIN loss: 0.159\n",
      "[Iteration 1150/15940] TRAIN loss: 0.194\n",
      "[Iteration 1200/15940] TRAIN loss: 0.154\n",
      "[Iteration 1250/15940] TRAIN loss: 0.200\n",
      "[Iteration 1300/15940] TRAIN loss: 0.114\n",
      "[Iteration 1350/15940] TRAIN loss: 0.169\n",
      "[Iteration 1400/15940] TRAIN loss: 0.131\n",
      "[Iteration 1450/15940] TRAIN loss: 0.144\n",
      "[Iteration 1500/15940] TRAIN loss: 0.153\n",
      "[Iteration 1550/15940] TRAIN loss: 0.121\n",
      "[Epoch 1/10] TRAIN acc/loss: 1.000/0.121\n",
      "[Epoch 1/10] VAL   acc/loss: 0.961/0.131\n",
      "[Iteration 1644/15940] TRAIN loss: 0.141\n",
      "[Iteration 1694/15940] TRAIN loss: 0.164\n",
      "[Iteration 1744/15940] TRAIN loss: 0.119\n",
      "[Iteration 1794/15940] TRAIN loss: 0.129\n",
      "[Iteration 1844/15940] TRAIN loss: 0.136\n",
      "[Iteration 1894/15940] TRAIN loss: 0.097\n",
      "[Iteration 1944/15940] TRAIN loss: 0.131\n",
      "[Iteration 1994/15940] TRAIN loss: 0.113\n",
      "[Iteration 2044/15940] TRAIN loss: 0.132\n",
      "[Iteration 2094/15940] TRAIN loss: 0.103\n",
      "[Iteration 2144/15940] TRAIN loss: 0.132\n",
      "[Iteration 2194/15940] TRAIN loss: 0.121\n",
      "[Iteration 2244/15940] TRAIN loss: 0.121\n",
      "[Iteration 2294/15940] TRAIN loss: 0.112\n",
      "[Iteration 2344/15940] TRAIN loss: 0.115\n",
      "[Iteration 2394/15940] TRAIN loss: 0.119\n",
      "[Iteration 2444/15940] TRAIN loss: 0.093\n",
      "[Iteration 2494/15940] TRAIN loss: 0.099\n",
      "[Iteration 2544/15940] TRAIN loss: 0.155\n",
      "[Iteration 2594/15940] TRAIN loss: 0.140\n",
      "[Iteration 2644/15940] TRAIN loss: 0.096\n",
      "[Iteration 2694/15940] TRAIN loss: 0.099\n",
      "[Iteration 2744/15940] TRAIN loss: 0.101\n",
      "[Iteration 2794/15940] TRAIN loss: 0.101\n",
      "[Iteration 2844/15940] TRAIN loss: 0.105\n",
      "[Iteration 2894/15940] TRAIN loss: 0.083\n",
      "[Iteration 2944/15940] TRAIN loss: 0.085\n",
      "[Iteration 2994/15940] TRAIN loss: 0.124\n",
      "[Iteration 3044/15940] TRAIN loss: 0.101\n",
      "[Iteration 3094/15940] TRAIN loss: 0.089\n",
      "[Iteration 3144/15940] TRAIN loss: 0.073\n",
      "[Epoch 2/10] TRAIN acc/loss: 1.000/0.073\n",
      "[Epoch 2/10] VAL   acc/loss: 0.969/0.101\n",
      "[Iteration 3238/15940] TRAIN loss: 0.076\n",
      "[Iteration 3288/15940] TRAIN loss: 0.074\n",
      "[Iteration 3338/15940] TRAIN loss: 0.098\n",
      "[Iteration 3388/15940] TRAIN loss: 0.079\n",
      "[Iteration 3438/15940] TRAIN loss: 0.086\n",
      "[Iteration 3488/15940] TRAIN loss: 0.065\n",
      "[Iteration 3538/15940] TRAIN loss: 0.059\n",
      "[Iteration 3588/15940] TRAIN loss: 0.098\n",
      "[Iteration 3638/15940] TRAIN loss: 0.097\n",
      "[Iteration 3688/15940] TRAIN loss: 0.094\n",
      "[Iteration 3738/15940] TRAIN loss: 0.100\n",
      "[Iteration 3788/15940] TRAIN loss: 0.074\n",
      "[Iteration 3838/15940] TRAIN loss: 0.096\n",
      "[Iteration 3888/15940] TRAIN loss: 0.091\n",
      "[Iteration 3938/15940] TRAIN loss: 0.070\n",
      "[Iteration 3988/15940] TRAIN loss: 0.054\n",
      "[Iteration 4038/15940] TRAIN loss: 0.066\n",
      "[Iteration 4088/15940] TRAIN loss: 0.074\n",
      "[Iteration 4138/15940] TRAIN loss: 0.097\n",
      "[Iteration 4188/15940] TRAIN loss: 0.094\n",
      "[Iteration 4238/15940] TRAIN loss: 0.070\n",
      "[Iteration 4288/15940] TRAIN loss: 0.061\n",
      "[Iteration 4338/15940] TRAIN loss: 0.067\n",
      "[Iteration 4388/15940] TRAIN loss: 0.076\n",
      "[Iteration 4438/15940] TRAIN loss: 0.082\n",
      "[Iteration 4488/15940] TRAIN loss: 0.059\n",
      "[Iteration 4538/15940] TRAIN loss: 0.097\n",
      "[Iteration 4588/15940] TRAIN loss: 0.062\n",
      "[Iteration 4638/15940] TRAIN loss: 0.065\n",
      "[Iteration 4688/15940] TRAIN loss: 0.067\n",
      "[Iteration 4738/15940] TRAIN loss: 0.082\n",
      "[Epoch 3/10] TRAIN acc/loss: 0.958/0.082\n",
      "[Epoch 3/10] VAL   acc/loss: 0.975/0.077\n",
      "[Iteration 4832/15940] TRAIN loss: 0.053\n",
      "[Iteration 4882/15940] TRAIN loss: 0.054\n",
      "[Iteration 4932/15940] TRAIN loss: 0.068\n",
      "[Iteration 4982/15940] TRAIN loss: 0.052\n",
      "[Iteration 5032/15940] TRAIN loss: 0.077\n",
      "[Iteration 5082/15940] TRAIN loss: 0.069\n",
      "[Iteration 5132/15940] TRAIN loss: 0.064\n",
      "[Iteration 5182/15940] TRAIN loss: 0.062\n",
      "[Iteration 5232/15940] TRAIN loss: 0.059\n",
      "[Iteration 5282/15940] TRAIN loss: 0.061\n",
      "[Iteration 5332/15940] TRAIN loss: 0.061\n",
      "[Iteration 5382/15940] TRAIN loss: 0.059\n",
      "[Iteration 5432/15940] TRAIN loss: 0.063\n",
      "[Iteration 5482/15940] TRAIN loss: 0.054\n",
      "[Iteration 5532/15940] TRAIN loss: 0.061\n",
      "[Iteration 5582/15940] TRAIN loss: 0.076\n",
      "[Iteration 5632/15940] TRAIN loss: 0.065\n",
      "[Iteration 5682/15940] TRAIN loss: 0.052\n",
      "[Iteration 5732/15940] TRAIN loss: 0.064\n",
      "[Iteration 5782/15940] TRAIN loss: 0.054\n",
      "[Iteration 5832/15940] TRAIN loss: 0.055\n",
      "[Iteration 5882/15940] TRAIN loss: 0.056\n",
      "[Iteration 5932/15940] TRAIN loss: 0.075\n",
      "[Iteration 5982/15940] TRAIN loss: 0.087\n",
      "[Iteration 6032/15940] TRAIN loss: 0.044\n",
      "[Iteration 6082/15940] TRAIN loss: 0.052\n",
      "[Iteration 6132/15940] TRAIN loss: 0.063\n",
      "[Iteration 6182/15940] TRAIN loss: 0.060\n",
      "[Iteration 6232/15940] TRAIN loss: 0.046\n",
      "[Iteration 6282/15940] TRAIN loss: 0.056\n",
      "[Iteration 6332/15940] TRAIN loss: 0.051\n",
      "[Epoch 4/10] TRAIN acc/loss: 0.917/0.051\n",
      "[Epoch 4/10] VAL   acc/loss: 0.981/0.058\n",
      "[Iteration 6426/15940] TRAIN loss: 0.052\n",
      "[Iteration 6476/15940] TRAIN loss: 0.038\n",
      "[Iteration 6526/15940] TRAIN loss: 0.048\n",
      "[Iteration 6576/15940] TRAIN loss: 0.038\n",
      "[Iteration 6626/15940] TRAIN loss: 0.048\n",
      "[Iteration 6676/15940] TRAIN loss: 0.048\n",
      "[Iteration 6726/15940] TRAIN loss: 0.043\n",
      "[Iteration 6776/15940] TRAIN loss: 0.050\n",
      "[Iteration 6826/15940] TRAIN loss: 0.058\n",
      "[Iteration 6876/15940] TRAIN loss: 0.068\n",
      "[Iteration 6926/15940] TRAIN loss: 0.044\n",
      "[Iteration 6976/15940] TRAIN loss: 0.059\n",
      "[Iteration 7026/15940] TRAIN loss: 0.049\n",
      "[Iteration 7076/15940] TRAIN loss: 0.053\n",
      "[Iteration 7126/15940] TRAIN loss: 0.056\n",
      "[Iteration 7176/15940] TRAIN loss: 0.052\n",
      "[Iteration 7226/15940] TRAIN loss: 0.044\n",
      "[Iteration 7276/15940] TRAIN loss: 0.054\n",
      "[Iteration 7326/15940] TRAIN loss: 0.029\n",
      "[Iteration 7376/15940] TRAIN loss: 0.045\n",
      "[Iteration 7426/15940] TRAIN loss: 0.050\n",
      "[Iteration 7476/15940] TRAIN loss: 0.072\n",
      "[Iteration 7526/15940] TRAIN loss: 0.063\n",
      "[Iteration 7576/15940] TRAIN loss: 0.052\n",
      "[Iteration 7626/15940] TRAIN loss: 0.037\n",
      "[Iteration 7676/15940] TRAIN loss: 0.049\n",
      "[Iteration 7726/15940] TRAIN loss: 0.052\n",
      "[Iteration 7776/15940] TRAIN loss: 0.072\n",
      "[Iteration 7826/15940] TRAIN loss: 0.067\n",
      "[Iteration 7876/15940] TRAIN loss: 0.058\n",
      "[Iteration 7926/15940] TRAIN loss: 0.053\n",
      "[Epoch 5/10] TRAIN acc/loss: 1.000/0.053\n",
      "[Epoch 5/10] VAL   acc/loss: 0.975/0.078\n",
      "[Iteration 8020/15940] TRAIN loss: 0.041\n",
      "[Iteration 8070/15940] TRAIN loss: 0.055\n",
      "[Iteration 8120/15940] TRAIN loss: 0.047\n",
      "[Iteration 8170/15940] TRAIN loss: 0.034\n",
      "[Iteration 8220/15940] TRAIN loss: 0.029\n",
      "[Iteration 8270/15940] TRAIN loss: 0.051\n",
      "[Iteration 8320/15940] TRAIN loss: 0.040\n",
      "[Iteration 8370/15940] TRAIN loss: 0.044\n",
      "[Iteration 8420/15940] TRAIN loss: 0.040\n",
      "[Iteration 8470/15940] TRAIN loss: 0.039\n",
      "[Iteration 8520/15940] TRAIN loss: 0.026\n",
      "[Iteration 8570/15940] TRAIN loss: 0.030\n",
      "[Iteration 8620/15940] TRAIN loss: 0.038\n",
      "[Iteration 8670/15940] TRAIN loss: 0.039\n",
      "[Iteration 8720/15940] TRAIN loss: 0.041\n",
      "[Iteration 8770/15940] TRAIN loss: 0.030\n",
      "[Iteration 8820/15940] TRAIN loss: 0.047\n",
      "[Iteration 8870/15940] TRAIN loss: 0.035\n",
      "[Iteration 8920/15940] TRAIN loss: 0.044\n",
      "[Iteration 8970/15940] TRAIN loss: 0.033\n",
      "[Iteration 9020/15940] TRAIN loss: 0.046\n",
      "[Iteration 9070/15940] TRAIN loss: 0.056\n",
      "[Iteration 9120/15940] TRAIN loss: 0.035\n",
      "[Iteration 9170/15940] TRAIN loss: 0.036\n",
      "[Iteration 9220/15940] TRAIN loss: 0.036\n",
      "[Iteration 9270/15940] TRAIN loss: 0.041\n",
      "[Iteration 9320/15940] TRAIN loss: 0.042\n",
      "[Iteration 9370/15940] TRAIN loss: 0.063\n",
      "[Iteration 9420/15940] TRAIN loss: 0.047\n",
      "[Iteration 9470/15940] TRAIN loss: 0.045\n",
      "[Iteration 9520/15940] TRAIN loss: 0.045\n",
      "[Epoch 6/10] TRAIN acc/loss: 0.958/0.045\n",
      "[Epoch 6/10] VAL   acc/loss: 0.987/0.044\n",
      "[Iteration 9614/15940] TRAIN loss: 0.038\n",
      "[Iteration 9664/15940] TRAIN loss: 0.030\n",
      "[Iteration 9714/15940] TRAIN loss: 0.036\n",
      "[Iteration 9764/15940] TRAIN loss: 0.032\n",
      "[Iteration 9814/15940] TRAIN loss: 0.044\n",
      "[Iteration 9864/15940] TRAIN loss: 0.035\n",
      "[Iteration 9914/15940] TRAIN loss: 0.030\n",
      "[Iteration 9964/15940] TRAIN loss: 0.034\n",
      "[Iteration 10014/15940] TRAIN loss: 0.036\n",
      "[Iteration 10064/15940] TRAIN loss: 0.048\n",
      "[Iteration 10114/15940] TRAIN loss: 0.029\n",
      "[Iteration 10164/15940] TRAIN loss: 0.041\n",
      "[Iteration 10214/15940] TRAIN loss: 0.032\n",
      "[Iteration 10264/15940] TRAIN loss: 0.050\n",
      "[Iteration 10314/15940] TRAIN loss: 0.048\n",
      "[Iteration 10364/15940] TRAIN loss: 0.026\n",
      "[Iteration 10414/15940] TRAIN loss: 0.047\n",
      "[Iteration 10464/15940] TRAIN loss: 0.035\n",
      "[Iteration 10514/15940] TRAIN loss: 0.035\n",
      "[Iteration 10564/15940] TRAIN loss: 0.045\n",
      "[Iteration 10614/15940] TRAIN loss: 0.041\n",
      "[Iteration 10664/15940] TRAIN loss: 0.035\n",
      "[Iteration 10714/15940] TRAIN loss: 0.033\n",
      "[Iteration 10764/15940] TRAIN loss: 0.027\n",
      "[Iteration 10814/15940] TRAIN loss: 0.036\n",
      "[Iteration 10864/15940] TRAIN loss: 0.059\n",
      "[Iteration 10914/15940] TRAIN loss: 0.030\n",
      "[Iteration 10964/15940] TRAIN loss: 0.031\n",
      "[Iteration 11014/15940] TRAIN loss: 0.033\n",
      "[Iteration 11064/15940] TRAIN loss: 0.030\n",
      "[Iteration 11114/15940] TRAIN loss: 0.035\n",
      "[Epoch 7/10] TRAIN acc/loss: 1.000/0.035\n",
      "[Epoch 7/10] VAL   acc/loss: 0.987/0.046\n",
      "[Iteration 11208/15940] TRAIN loss: 0.030\n",
      "[Iteration 11258/15940] TRAIN loss: 0.026\n",
      "[Iteration 11308/15940] TRAIN loss: 0.035\n",
      "[Iteration 11358/15940] TRAIN loss: 0.031\n",
      "[Iteration 11408/15940] TRAIN loss: 0.027\n",
      "[Iteration 11458/15940] TRAIN loss: 0.036\n",
      "[Iteration 11508/15940] TRAIN loss: 0.022\n",
      "[Iteration 11558/15940] TRAIN loss: 0.025\n",
      "[Iteration 11608/15940] TRAIN loss: 0.035\n",
      "[Iteration 11658/15940] TRAIN loss: 0.029\n",
      "[Iteration 11708/15940] TRAIN loss: 0.019\n",
      "[Iteration 11758/15940] TRAIN loss: 0.032\n",
      "[Iteration 11808/15940] TRAIN loss: 0.030\n",
      "[Iteration 11858/15940] TRAIN loss: 0.046\n",
      "[Iteration 11908/15940] TRAIN loss: 0.060\n",
      "[Iteration 11958/15940] TRAIN loss: 0.038\n",
      "[Iteration 12008/15940] TRAIN loss: 0.029\n",
      "[Iteration 12058/15940] TRAIN loss: 0.028\n",
      "[Iteration 12108/15940] TRAIN loss: 0.027\n",
      "[Iteration 12158/15940] TRAIN loss: 0.037\n",
      "[Iteration 12208/15940] TRAIN loss: 0.038\n",
      "[Iteration 12258/15940] TRAIN loss: 0.027\n",
      "[Iteration 12308/15940] TRAIN loss: 0.028\n",
      "[Iteration 12358/15940] TRAIN loss: 0.050\n",
      "[Iteration 12408/15940] TRAIN loss: 0.033\n",
      "[Iteration 12458/15940] TRAIN loss: 0.035\n",
      "[Iteration 12508/15940] TRAIN loss: 0.034\n",
      "[Iteration 12558/15940] TRAIN loss: 0.033\n",
      "[Iteration 12608/15940] TRAIN loss: 0.046\n",
      "[Iteration 12658/15940] TRAIN loss: 0.036\n",
      "[Iteration 12708/15940] TRAIN loss: 0.027\n",
      "[Epoch 8/10] TRAIN acc/loss: 1.000/0.027\n",
      "[Epoch 8/10] VAL   acc/loss: 0.988/0.041\n",
      "[Iteration 12802/15940] TRAIN loss: 0.024\n",
      "[Iteration 12852/15940] TRAIN loss: 0.027\n",
      "[Iteration 12902/15940] TRAIN loss: 0.032\n",
      "[Iteration 12952/15940] TRAIN loss: 0.036\n",
      "[Iteration 13002/15940] TRAIN loss: 0.026\n",
      "[Iteration 13052/15940] TRAIN loss: 0.025\n",
      "[Iteration 13102/15940] TRAIN loss: 0.033\n",
      "[Iteration 13152/15940] TRAIN loss: 0.017\n",
      "[Iteration 13202/15940] TRAIN loss: 0.014\n",
      "[Iteration 13252/15940] TRAIN loss: 0.022\n",
      "[Iteration 13302/15940] TRAIN loss: 0.025\n",
      "[Iteration 13352/15940] TRAIN loss: 0.033\n",
      "[Iteration 13402/15940] TRAIN loss: 0.029\n",
      "[Iteration 13452/15940] TRAIN loss: 0.051\n",
      "[Iteration 13502/15940] TRAIN loss: 0.023\n",
      "[Iteration 13552/15940] TRAIN loss: 0.026\n",
      "[Iteration 13602/15940] TRAIN loss: 0.029\n",
      "[Iteration 13652/15940] TRAIN loss: 0.030\n",
      "[Iteration 13702/15940] TRAIN loss: 0.036\n",
      "[Iteration 13752/15940] TRAIN loss: 0.029\n",
      "[Iteration 13802/15940] TRAIN loss: 0.027\n",
      "[Iteration 13852/15940] TRAIN loss: 0.031\n",
      "[Iteration 13902/15940] TRAIN loss: 0.023\n",
      "[Iteration 13952/15940] TRAIN loss: 0.029\n",
      "[Iteration 14002/15940] TRAIN loss: 0.024\n",
      "[Iteration 14052/15940] TRAIN loss: 0.028\n",
      "[Iteration 14102/15940] TRAIN loss: 0.025\n",
      "[Iteration 14152/15940] TRAIN loss: 0.030\n",
      "[Iteration 14202/15940] TRAIN loss: 0.029\n",
      "[Iteration 14252/15940] TRAIN loss: 0.041\n",
      "[Iteration 14302/15940] TRAIN loss: 0.041\n",
      "[Epoch 9/10] TRAIN acc/loss: 1.000/0.041\n",
      "[Epoch 9/10] VAL   acc/loss: 0.986/0.049\n",
      "[Iteration 14396/15940] TRAIN loss: 0.026\n",
      "[Iteration 14446/15940] TRAIN loss: 0.035\n",
      "[Iteration 14496/15940] TRAIN loss: 0.021\n",
      "[Iteration 14546/15940] TRAIN loss: 0.017\n",
      "[Iteration 14596/15940] TRAIN loss: 0.028\n",
      "[Iteration 14646/15940] TRAIN loss: 0.029\n",
      "[Iteration 14696/15940] TRAIN loss: 0.017\n",
      "[Iteration 14746/15940] TRAIN loss: 0.018\n",
      "[Iteration 14796/15940] TRAIN loss: 0.026\n",
      "[Iteration 14846/15940] TRAIN loss: 0.017\n",
      "[Iteration 14896/15940] TRAIN loss: 0.029\n",
      "[Iteration 14946/15940] TRAIN loss: 0.028\n",
      "[Iteration 14996/15940] TRAIN loss: 0.027\n",
      "[Iteration 15046/15940] TRAIN loss: 0.021\n",
      "[Iteration 15096/15940] TRAIN loss: 0.023\n",
      "[Iteration 15146/15940] TRAIN loss: 0.018\n",
      "[Iteration 15196/15940] TRAIN loss: 0.028\n",
      "[Iteration 15246/15940] TRAIN loss: 0.031\n",
      "[Iteration 15296/15940] TRAIN loss: 0.043\n",
      "[Iteration 15346/15940] TRAIN loss: 0.034\n",
      "[Iteration 15396/15940] TRAIN loss: 0.028\n",
      "[Iteration 15446/15940] TRAIN loss: 0.037\n",
      "[Iteration 15496/15940] TRAIN loss: 0.028\n",
      "[Iteration 15546/15940] TRAIN loss: 0.018\n",
      "[Iteration 15596/15940] TRAIN loss: 0.015\n",
      "[Iteration 15646/15940] TRAIN loss: 0.027\n",
      "[Iteration 15696/15940] TRAIN loss: 0.028\n",
      "[Iteration 15746/15940] TRAIN loss: 0.022\n",
      "[Iteration 15796/15940] TRAIN loss: 0.032\n",
      "[Iteration 15846/15940] TRAIN loss: 0.026\n",
      "[Iteration 15896/15940] TRAIN loss: 0.034\n",
      "[Epoch 10/10] TRAIN acc/loss: 1.000/0.034\n",
      "[Epoch 10/10] VAL   acc/loss: 0.986/0.048\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.rnn.rnn_nn import LSTM_Classifier\n",
    "model= LSTM_Classifier()\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_dset,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                dataset=val_dset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "solver = Solver(optim_args={\"lr\": 1e-3})\n",
    "\n",
    "# train rnn model\n",
    "solver.train(model, train_loader, val_loader, log_nth=50, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your LSTM model again and see wether it improves performance on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your Model\n",
    "When you are satisfied with your training, you can save the model. In order to be eligible for the bonus points you have to achieve a score higher than __97__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "When you are satisfied with your training, you can save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model... models/rnn_mnist_nn.model\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "model_rnn.save(\"models/rnn_mnist_nn.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 TF",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
